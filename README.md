
# ðŸ“˜ Dockerized Data Pipeline Project  

> A **simple yet complete data pipeline project** built with **Docker**, **Apache Airflow**, **PostgreSQL**, and **Python** â€” designed to demonstrate core concepts clearly for learners, technical managers, and recruiters.

---

## âš™ï¸ Project Overview  

This project shows how multiple services can work together inside Docker containers:  

- ðŸ **Python scripts** â†’ e.g., `scraper.py`, `books_pipeline.py`  
- ðŸ—„ï¸ **Database** â†’ PostgreSQL  
- ðŸ” **Workflow orchestrator** â†’ Apache Airflow  
- ðŸ“Š *(Optional)* **Visualization** â†’ Apache Superset  

Everything runs seamlessly in containers, making it reproducible on any system with Docker installed.

---

## ðŸ§© Step-by-Step Execution  

### 1ï¸âƒ£ Prepare Project Structure  
Organize your code and configuration into clean folders:
```
app/
analytics/
dags/
```

Each component (scraper, database, Airflow) lives in its own folder for clarity.

---

### 2ï¸âƒ£ Write the Dockerfile  
Each service gets its own `Dockerfile` describing how to build and run it.

Example:
```dockerfile
FROM python:3.10
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "scraper.py"]
```

---

### 3ï¸âƒ£ Define Services in docker-compose.yml â¤ï¸  
This is the heart of the project â€” it connects all containers together:

- **postgres** â†’ database  
- **airflow-*** â†’ Airflow scheduler, webserver, worker, triggerer  
- *(Optional)* **Flask or Scraper** service  

Then simply run:
```bash
docker compose up -d
```

---

### 4ï¸âƒ£ Connect the Services  
- Airflow connects to PostgreSQL using a **Connection ID** (e.g., `postgres_default`).  
- The **DAG** (`books_pipeline.py`) triggers the scraper and stores results into PostgreSQL.  
- Superset (optional) can later visualize these results.

---

### 5ï¸âƒ£ Test and Manage  
Everything runs inside containers, so you can test easily:  

- Access **Airflow UI** â†’ check if DAGs are loaded and active.  
- Inspect **PostgreSQL** â†’ confirm data insertion.  

---

## ðŸ“ Project Structure  

```
.dockerignore
.env
.flaskenv
.gitignore
create_tables.py
docker-compose.yml
requirements.txt
run.py
README.md
```

### app/
```
api.py
build_app.py
models.py
scraper.py
Dockerfile
__init__.py
```

### analytics/
```
spark_postgres_analysis.py
max_salebook.py
test_pandas_pyarrow.py
```

### dags/
```
scraper_airflow.py
save_to_db.py
books_pipeline.py
```

---

## ðŸ§  Common Issues & Fixes  

| âš ï¸ Problem | ðŸ’¡ Solution | ðŸ§© Explanation |
|-------------|--------------|----------------|
| `ModuleNotFoundError: No module named 'scraper'` | Copy scraper files into the DAGs folder and fix import paths. | ðŸ“š *â€œThe books were hiding â€” we invited them into Airflow city.â€* |
| DAG not visible in Airflow UI | Rename the DAG file correctly and clear cache. | ðŸ‘€ *â€œThe DAG was playing hide-and-seek â€” renaming made it wave hello.â€* |
| `ModuleNotFoundError: No module named 'app'` | Move database/model logic into a standalone file like `models_airflow.py`. | ðŸ—ï¸ *â€œThe models were stuck in the Flask castle â€” now they live in Airflow.â€* |
| `password authentication failed for user "postgres"` | Sync credentials with `.env` and `docker-compose.yml`. | ðŸ”‘ *â€œPostgreSQL finally accepted our right keys.â€* |
| DAG execution error | Verify all containers are running and dependencies exist in the DAGs folder. | ðŸš€ *â€œThe DAGs were sleeping â€” we woke them up by organizing their files.â€* |

---

## ðŸ§© Technical Highlights  

- **PostgresHook** in Airflow handles PostgreSQL connections elegantly via `Connection ID`.  
- No hardcoded credentials â€” everything is securely managed by Airflow.  
- The `Book` model contains only essential fields:  
  ```python
  id, title, price, availability
  ```  
- `Base.metadata.create_all(bind=engine)` automatically creates tables if missing.  

---

## ðŸŽ¯ Final Result  

âœ… Fully containerized data pipeline  
âœ… Portable across any system with Docker  
âœ… Minimal, clean, and understandable codebase  
âœ… Perfect for showing your understanding of **Docker**, **Airflow**, and **data engineering fundamentals**

---

## ðŸ’¬ Why I Built This Project  

I built this project as a **practical learning journey** to understand how modern data pipelines work â€” from data extraction to storage, orchestration, and visualization â€” using real tools used by data engineers in production.  

My goal was **not just to make it work**, but to make it **clear, reproducible, and educational** â€” so anyone (even non-developers) can open the project, run one command, and see a complete data pipeline in action.  

Through this project, I learned how to:  
- ðŸ§© Structure services inside Docker containers  
- ðŸ” Orchestrate workflows using Apache Airflow  
- ðŸ—„ï¸ Store and query data in PostgreSQL  
- âš™ï¸ Debug common container issues  
- ðŸ§  Keep everything simple, modular, and production-friendly  

This project reflects my **transition from a marketing professional to a data engineer**, showing my curiosity, persistence, and real understanding of how data flows through modern systems.  

> ðŸš€ I believe simplicity is the ultimate sophistication â€” and thatâ€™s exactly what this project represents.  
